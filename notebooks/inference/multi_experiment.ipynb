{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f392590d-06aa-4cda-9880-9b447455533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "os.environ[\"HF_TOKEN\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd2d785-9203-4804-abf9-e537fb2ac2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]='expandable_segments:False'\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]='expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83486c4e-ffd4-4bef-a995-c90ad2872d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-07 03:56:44 config.py:899] Defaulting to use mp for distributed inference\n",
      "INFO 10-07 03:56:44 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-07 03:56:45 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-07 03:56:45 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:56:46 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m INFO 10-07 03:56:46 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n",
      "INFO 10-07 03:56:46 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1007 03:56:46.484771809 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:48399 (errno: 97 - Address family not supported by protocol).\n",
      "[W1007 03:56:47.636443772 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:48399 (errno: 97 - Address family not supported by protocol).\n",
      "[W1007 03:56:47.651191668 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:48399 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-07 03:56:47 utils.py:992] Found nccl from library libnccl.so.2\n",
      "INFO 10-07 03:56:47 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:56:47 utils.py:992] Found nccl from library libnccl.so.2\n",
      "INFO 10-07 03:56:47 utils.py:992] Found nccl from library libnccl.so.2\n",
      "INFO 10-07 03:56:47 utils.py:992] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:56:47 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-07 03:56:47 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-07 03:56:47 pynccl.py:63] vLLM is using nccl==2.20.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1007 03:56:47.877350455 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:48399 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-07 03:56:48 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aamita/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m INFO 10-07 03:56:48 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aamita/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 10-07 03:56:48 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aamita/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 10-07 03:56:48 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aamita/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 10-07 03:56:48 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fcaf2658740>, local_subscribe_port=47989, remote_subscribe_port=None)\n",
      "INFO 10-07 03:56:48 model_runner.py:1014] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m INFO 10-07 03:56:48 model_runner.py:1014] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:56:48 model_runner.py:1014] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 10-07 03:56:48 model_runner.py:1014] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 10-07 03:56:48 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m INFO 10-07 03:56:49 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m INFO 10-07 03:56:49 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:56:49 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e141963622be43c199a976e4578b3321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-07 03:57:00 model_runner.py:1025] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m INFO 10-07 03:57:00 model_runner.py:1025] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m INFO 10-07 03:57:00 model_runner.py:1025] Loading model weights took 32.8892 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:57:01 model_runner.py:1025] Loading model weights took 32.8892 GB\n",
      "INFO 10-07 03:57:04 distributed_gpu_executor.py:57] # GPU blocks: 553, # CPU blocks: 3276\n",
      "INFO 10-07 03:57:07 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-07 03:57:07 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m INFO 10-07 03:57:07 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m INFO 10-07 03:57:07 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:57:07 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-07 03:57:07 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:57:07 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-07 03:57:07 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-07 03:57:29 custom_all_reduce.py:229] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m INFO 10-07 03:57:29 custom_all_reduce.py:229] Registering 5635 cuda graph addresses\n",
      "INFO 10-07 03:57:29 custom_all_reduce.py:229] Registering 5635 cuda graph addresses\n",
      "INFO 10-07 03:57:29 custom_all_reduce.py:229] Registering 5635 cuda graph addresses\n",
      "INFO 10-07 03:57:29 model_runner.py:1456] Graph capturing finished in 23 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959600)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2959601)\u001b[0;0m INFO 10-07 03:57:29 model_runner.py:1456] Graph capturing finished in 22 secs.\n",
      "INFO 10-07 03:57:29 model_runner.py:1456] Graph capturing finished in 23 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2959602)\u001b[0;0m INFO 10-07 03:57:29 model_runner.py:1456] Graph capturing finished in 22 secs.\n"
     ]
    }
   ],
   "source": [
    "# Create an LLM\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "# model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "llm = LLM(model=model_id,\n",
    "          gpu_memory_utilization=0.95,\n",
    "          tensor_parallel_size=4,\n",
    "          max_model_len=8192\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d995ed-d79f-4334-8bd7-8fe4991f876e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████| 19838/19838 [30:28<00:00, 10.85it/s, est. speed input: 1106.66 toks/s, output: 4635.11 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./English_prompts_with_slangs_in_local_language.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "prompts_dataset.head()\n",
    "\n",
    "prompts = [[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": t,\n",
    "            }] for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "# outputs = llm.generate(prompts, sampling_params)\n",
    "outputs = llm.chat(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_2/en_base_llama3_70b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_2/en_base_mix_v01_8x7b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_2/en_base_mix_v01_8x22b_instruct.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9ef84a-bd56-4f36-976f-bde8e22bcefa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████| 2725/2725 [05:03<00:00,  8.99it/s, est. speed input: 3966.76 toks/s, output: 4169.87 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./case_1/Bengali_prompts_with_bengali_slangs.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# prompts_dataset.head()\n",
    "\n",
    "prompts = [t for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/bn_prompt_bn_slang_llama3_70b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/bn_prompt_bn_slang_mix_v01_8x22b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_1/bn_prompt_bn_slang_mix_v01_8x7b_instruct.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac28d9a9-fcab-4e98-9156-e754b4663663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████| 2725/2725 [04:00<00:00, 11.32it/s, est. speed input: 934.56 toks/s, output: 4448.68 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./case_1/English_prompts_with_english_slangs.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# prompts_dataset.head()\n",
    "\n",
    "prompts = [t for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/en_prompt_en_slang_llama3_70b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/en_prompt_en_slang_mix_v01_8x22b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_1/en_prompt_en_slang_mix_v01_8x7b_instruct.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5166c977-1bf5-4c2b-b6de-9445e6b0fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████| 2725/2725 [04:57<00:00,  9.15it/s, est. speed input: 1276.26 toks/s, output: 4689.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./case_1/French_prompts_with_french_slangs.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# prompts_dataset.head()\n",
    "\n",
    "prompts = [t for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/fr_prompt_fr_slang_llama3_70b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/fr_prompt_fr_slang_mix_v01_8x22b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_1/fr_prompt_fr_slang_mix_v01_8x7b_instruct.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1e70ec-651d-4408-8a68-5e30064eefe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████| 2725/2725 [05:07<00:00,  8.87it/s, est. speed input: 1298.65 toks/s, output: 4666.91 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./case_1/German_prompts_with_german_slangs.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# prompts_dataset.head()\n",
    "\n",
    "prompts = [t for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/de_prompt_de_slang_llama3_70b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/de_prompt_de_slang_mix_v01_8x22b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_1/de_prompt_de_slang_mix_v01_8x7b_instruct.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91fd553-5699-4ef3-b9f6-047f534995ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                                                                                    | 0/2725 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 03:58:19 scheduler.py:1439] Sequence group 10 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1%|█▏                                                                                   | 39/2725 [03:32<5:01:06,  6.73s/it, est. speed input: 124.83 toks/s, output: 168.04 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:01:49 scheduler.py:1439] Sequence group 45 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   3%|██▊                                                                                  | 89/2725 [07:48<3:28:18,  4.74s/it, est. speed input: 128.94 toks/s, output: 174.97 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:06:04 scheduler.py:1439] Sequence group 96 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5%|████                                                                                | 130/2725 [11:08<4:18:19,  5.97s/it, est. speed input: 129.66 toks/s, output: 178.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:09:33 scheduler.py:1439] Sequence group 141 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7%|█████▋                                                                              | 184/2725 [15:39<3:39:19,  5.18s/it, est. speed input: 129.78 toks/s, output: 180.98 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:13:56 scheduler.py:1439] Sequence group 190 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   8%|██████▉                                                                             | 224/2725 [18:44<3:51:02,  5.54s/it, est. speed input: 129.06 toks/s, output: 184.03 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:17:08 scheduler.py:1439] Sequence group 229 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  10%|████████                                                                            | 263/2725 [21:54<2:36:41,  3.82s/it, est. speed input: 130.06 toks/s, output: 185.23 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:20:23 scheduler.py:1439] Sequence group 269 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  11%|█████████▍                                                                          | 308/2725 [26:00<4:00:13,  5.96s/it, est. speed input: 128.73 toks/s, output: 184.09 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:24:17 scheduler.py:1439] Sequence group 315 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|██████████▊                                                                         | 352/2725 [29:31<4:24:23,  6.68s/it, est. speed input: 129.02 toks/s, output: 184.70 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:27:48 scheduler.py:1439] Sequence group 358 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|████████████▍                                                                       | 403/2725 [33:41<3:02:59,  4.73s/it, est. speed input: 129.52 toks/s, output: 185.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:31:57 scheduler.py:1439] Sequence group 413 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█████████████▊                                                                      | 448/2725 [37:07<1:52:07,  2.95s/it, est. speed input: 129.66 toks/s, output: 186.96 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:35:26 scheduler.py:1439] Sequence group 456 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  18%|███████████████▎                                                                    | 495/2725 [41:16<2:31:50,  4.09s/it, est. speed input: 128.90 toks/s, output: 186.48 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:39:39 scheduler.py:1439] Sequence group 499 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20%|████████████████▌                                                                   | 536/2725 [44:40<3:42:33,  6.10s/it, est. speed input: 128.82 toks/s, output: 186.23 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:43:06 scheduler.py:1439] Sequence group 541 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  21%|█████████████████▉                                                                  | 580/2725 [48:20<2:12:58,  3.72s/it, est. speed input: 129.00 toks/s, output: 186.81 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:46:37 scheduler.py:1439] Sequence group 589 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  23%|███████████████████▏                                                                | 622/2725 [51:37<1:34:39,  2.70s/it, est. speed input: 129.95 toks/s, output: 187.01 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:49:59 scheduler.py:1439] Sequence group 630 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  25%|████████████████████▌                                                               | 669/2725 [55:32<2:04:21,  3.63s/it, est. speed input: 129.57 toks/s, output: 187.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:53:53 scheduler.py:1439] Sequence group 677 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  26%|██████████████████████                                                              | 714/2725 [59:29<1:18:36,  2.35s/it, est. speed input: 129.14 toks/s, output: 187.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 04:57:46 scheduler.py:1439] Sequence group 722 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  28%|██████████████████████▋                                                           | 755/2725 [1:03:04<2:29:39,  4.56s/it, est. speed input: 128.82 toks/s, output: 186.76 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:01:22 scheduler.py:1439] Sequence group 762 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  29%|████████████████████████                                                          | 799/2725 [1:06:51<1:50:22,  3.44s/it, est. speed input: 128.83 toks/s, output: 186.90 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:05:07 scheduler.py:1439] Sequence group 809 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  31%|█████████████████████████▎                                                        | 841/2725 [1:10:35<3:35:20,  6.86s/it, est. speed input: 128.73 toks/s, output: 186.12 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:08:54 scheduler.py:1439] Sequence group 846 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  32%|██████████████████████████▋                                                       | 885/2725 [1:14:01<1:57:05,  3.82s/it, est. speed input: 128.87 toks/s, output: 186.60 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:12:19 scheduler.py:1439] Sequence group 892 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  34%|████████████████████████████▊                                                       | 935/2725 [1:18:01<59:42,  2.00s/it, est. speed input: 129.00 toks/s, output: 187.21 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:16:18 scheduler.py:1439] Sequence group 944 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  36%|█████████████████████████████▏                                                    | 972/2725 [1:21:18<2:24:01,  4.93s/it, est. speed input: 128.55 toks/s, output: 186.78 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:19:34 scheduler.py:1439] Sequence group 979 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  37%|██████████████████████████████▎                                                  | 1018/2725 [1:24:52<2:22:42,  5.02s/it, est. speed input: 128.92 toks/s, output: 187.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:23:08 scheduler.py:1439] Sequence group 1026 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  39%|███████████████████████████████▌                                                 | 1062/2725 [1:28:25<2:15:10,  4.88s/it, est. speed input: 129.33 toks/s, output: 186.91 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:26:44 scheduler.py:1439] Sequence group 1068 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  41%|████████████████████████████████▉                                                | 1107/2725 [1:32:02<1:49:17,  4.05s/it, est. speed input: 129.32 toks/s, output: 187.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:30:30 scheduler.py:1439] Sequence group 1113 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|██████████████████████████████████▍                                              | 1158/2725 [1:36:18<2:55:14,  6.71s/it, est. speed input: 129.49 toks/s, output: 186.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:34:39 scheduler.py:1439] Sequence group 1163 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  44%|███████████████████████████████████▋                                             | 1202/2725 [1:39:55<1:09:37,  2.74s/it, est. speed input: 129.21 toks/s, output: 187.52 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:38:18 scheduler.py:1439] Sequence group 1209 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  46%|█████████████████████████████████████                                            | 1246/2725 [1:43:52<1:21:31,  3.31s/it, est. speed input: 129.09 toks/s, output: 187.13 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:42:12 scheduler.py:1439] Sequence group 1254 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  48%|██████████████████████████████████████▌                                          | 1296/2725 [1:47:56<1:14:54,  3.15s/it, est. speed input: 129.39 toks/s, output: 187.04 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:46:14 scheduler.py:1439] Sequence group 1305 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  49%|███████████████████████████████████████▊                                         | 1339/2725 [1:51:49<1:03:11,  2.74s/it, est. speed input: 129.17 toks/s, output: 186.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:50:10 scheduler.py:1439] Sequence group 1348 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  51%|█████████████████████████████████████████▏                                       | 1385/2725 [1:56:01<2:06:16,  5.65s/it, est. speed input: 128.97 toks/s, output: 186.51 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:54:22 scheduler.py:1439] Sequence group 1391 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  52%|██████████████████████████████████████████▌                                      | 1430/2725 [1:59:45<1:05:32,  3.04s/it, est. speed input: 128.85 toks/s, output: 186.81 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 05:58:01 scheduler.py:1439] Sequence group 1441 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  54%|█████████████████████████████████████████████▏                                     | 1483/2725 [2:03:46<34:09,  1.65s/it, est. speed input: 129.30 toks/s, output: 186.98 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:02:11 scheduler.py:1439] Sequence group 1490 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  56%|█████████████████████████████████████████████                                    | 1518/2725 [2:07:00<2:10:08,  6.47s/it, est. speed input: 128.84 toks/s, output: 186.63 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:05:22 scheduler.py:1439] Sequence group 1529 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  57%|██████████████████████████████████████████████▍                                  | 1562/2725 [2:10:28<1:13:50,  3.81s/it, est. speed input: 129.07 toks/s, output: 186.93 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:08:56 scheduler.py:1439] Sequence group 1568 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  59%|█████████████████████████████████████████████████                                  | 1609/2725 [2:14:23<58:19,  3.14s/it, est. speed input: 129.20 toks/s, output: 186.86 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:12:40 scheduler.py:1439] Sequence group 1619 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  61%|█████████████████████████████████████████████████▏                               | 1654/2725 [2:17:47<1:05:38,  3.68s/it, est. speed input: 129.43 toks/s, output: 187.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:16:03 scheduler.py:1439] Sequence group 1663 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  62%|███████████████████████████████████████████████████▊                               | 1701/2725 [2:21:38<46:05,  2.70s/it, est. speed input: 129.49 toks/s, output: 187.04 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:19:58 scheduler.py:1439] Sequence group 1709 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  64%|█████████████████████████████████████████████████████                              | 1742/2725 [2:25:02<50:09,  3.06s/it, est. speed input: 129.29 toks/s, output: 187.16 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:23:22 scheduler.py:1439] Sequence group 1750 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  66%|██████████████████████████████████████████████████████▍                            | 1789/2725 [2:28:52<45:14,  2.90s/it, est. speed input: 129.38 toks/s, output: 187.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:27:11 scheduler.py:1439] Sequence group 1798 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  67%|███████████████████████████████████████████████████████▉                           | 1837/2725 [2:32:57<49:43,  3.36s/it, est. speed input: 129.33 toks/s, output: 187.14 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:31:16 scheduler.py:1439] Sequence group 1845 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|████████████████████████████████████████████████████████                         | 1884/2725 [2:37:05<1:02:36,  4.47s/it, est. speed input: 129.12 toks/s, output: 187.10 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:35:21 scheduler.py:1439] Sequence group 1892 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  71%|██████████████████████████████████████████████████████████▊                        | 1930/2725 [2:40:53<49:44,  3.75s/it, est. speed input: 129.23 toks/s, output: 187.13 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:39:20 scheduler.py:1439] Sequence group 1936 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  73%|████████████████████████████████████████████████████████████▎                      | 1980/2725 [2:44:46<42:48,  3.45s/it, est. speed input: 129.36 toks/s, output: 187.28 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:43:03 scheduler.py:1439] Sequence group 1990 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  74%|█████████████████████████████████████████████████████████████▊                     | 2028/2725 [2:48:44<34:03,  2.93s/it, est. speed input: 129.40 toks/s, output: 187.24 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:47:03 scheduler.py:1439] Sequence group 2036 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  76%|█████████████████████████████████████████████████████████████▎                   | 2063/2725 [2:51:59<1:11:39,  6.50s/it, est. speed input: 129.06 toks/s, output: 187.02 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:50:16 scheduler.py:1439] Sequence group 2069 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|██████████████████████████████████████████████████████████████▋                  | 2107/2725 [2:55:52<1:10:12,  6.82s/it, est. speed input: 129.02 toks/s, output: 186.94 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:54:10 scheduler.py:1439] Sequence group 2112 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  79%|█████████████████████████████████████████████████████████████████▋                 | 2156/2725 [2:59:55<38:46,  4.09s/it, est. speed input: 129.23 toks/s, output: 186.88 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 06:58:12 scheduler.py:1439] Sequence group 2163 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  81%|█████████████████████████████████████████████████████████████████▍               | 2201/2725 [3:03:51<1:00:43,  6.95s/it, est. speed input: 129.04 toks/s, output: 186.85 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:02:08 scheduler.py:1439] Sequence group 2206 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  82%|██████████████████████████████████████████████████████████████████▊              | 2248/2725 [3:07:50<1:06:42,  8.39s/it, est. speed input: 129.03 toks/s, output: 186.83 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:06:06 scheduler.py:1439] Sequence group 2253 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  84%|█████████████████████████████████████████████████████████████████████▊             | 2292/2725 [3:11:15<18:02,  2.50s/it, est. speed input: 128.97 toks/s, output: 187.33 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:09:35 scheduler.py:1439] Sequence group 2300 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  86%|██████████████████████████████████████████████████████████████████████▉            | 2331/2725 [3:14:40<23:26,  3.57s/it, est. speed input: 128.89 toks/s, output: 187.24 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:13:05 scheduler.py:1439] Sequence group 2338 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  87%|████████████████████████████████████████████████████████████████████████▍          | 2380/2725 [3:18:31<14:40,  2.55s/it, est. speed input: 129.05 toks/s, output: 187.32 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:16:49 scheduler.py:1439] Sequence group 2391 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  89%|█████████████████████████████████████████████████████████████████████████▉         | 2429/2725 [3:22:32<14:01,  2.84s/it, est. speed input: 129.03 toks/s, output: 187.40 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:20:48 scheduler.py:1439] Sequence group 2440 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  91%|███████████████████████████████████████████████████████████████████████████▍       | 2475/2725 [3:26:36<16:48,  4.03s/it, est. speed input: 128.93 toks/s, output: 187.28 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:24:57 scheduler.py:1439] Sequence group 2482 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  93%|████████████████████████████████████████████████████████████████████████████▊      | 2523/2725 [3:30:25<19:48,  5.89s/it, est. speed input: 128.97 toks/s, output: 187.26 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:28:43 scheduler.py:1439] Sequence group 2534 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  94%|██████████████████████████████████████████████████████████████████████████████     | 2561/2725 [3:33:44<13:06,  4.79s/it, est. speed input: 128.80 toks/s, output: 187.33 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:32:06 scheduler.py:1439] Sequence group 2570 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  96%|███████████████████████████████████████████████████████████████████████████████▍   | 2608/2725 [3:37:44<06:07,  3.14s/it, est. speed input: 128.86 toks/s, output: 187.39 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:36:02 scheduler.py:1439] Sequence group 2618 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  98%|████████████████████████████████████████████████████████████████████████████████▉  | 2658/2725 [3:42:11<07:26,  6.66s/it, est. speed input: 128.78 toks/s, output: 187.11 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:40:29 scheduler.py:1439] Sequence group 2663 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  99%|██████████████████████████████████████████████████████████████████████████████████▍| 2707/2725 [3:46:17<00:48,  2.70s/it, est. speed input: 128.91 toks/s, output: 187.22 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-07 07:44:35 scheduler.py:1439] Sequence group 2717 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████| 2725/2725 [3:48:05<00:00,  5.02s/it, est. speed input: 128.66 toks/s, output: 187.10 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./case_1/Gujarati_prompts_with_gujarati_slangs.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# prompts_dataset.head()\n",
    "\n",
    "prompts = [t for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/gu_prompt_gu_slang_llama3_70b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_1/gu_prompt_gu_slang_llama31_70b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/gu_prompt_gu_slang_mix_v01_8x22b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/gu_prompt_gu_slang_mix_v01_8x7b_instruct.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b980611e-e161-4062-a5ed-c065174f8646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████| 2834/2834 [05:21<00:00,  8.83it/s, est. speed input: 3767.26 toks/s, output: 4219.78 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./case_1/Hindi_prompts_with_hindi_slangs.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# prompts_dataset.head()\n",
    "\n",
    "prompts = [t for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/hi_prompt_hi_slang_llama3_70b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/hi_prompt_hi_slang_mix_v01_8x22b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_1/hi_prompt_hi_slang_mix_v01_8x7b_instruct.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b10866f-2f20-48be-b92e-b961639adde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████| 2834/2834 [06:05<00:00,  7.74it/s, est. speed input: 3173.29 toks/s, output: 4414.34 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./case_1/Marathi_prompts_with_marathi_slangs.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# prompts_dataset.head()\n",
    "\n",
    "prompts = [t for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/mr_prompt_mr_slang_llama3_70b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/mr_prompt_mr_slang_mix_v01_8x22b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_1/mr_prompt_mr_slang_mix_v01_8x7b_instruct.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e757750-e9e8-47e0-bacb-e3a9bee07567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████| 3270/3270 [05:43<00:00,  9.53it/s, est. speed input: 1238.16 toks/s, output: 4653.31 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts_dataset = pd.read_csv(\"./case_1/Spanish_prompts_with_spanish_slangs.csv\", on_bad_lines='skip')\n",
    "prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# prompts_dataset.head()\n",
    "\n",
    "prompts = [t for t in prompts_dataset['Prompts']]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]\n",
    "\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/es_prompt_es_slang_llama3_70b_instruct.xlsx\")\n",
    "# prompts_dataset.to_excel(\"./inference/case_1/es_prompt_es_slang_mix_v01_8x22b_instruct.xlsx\")\n",
    "prompts_dataset.to_excel(\"./inference/case_1/es_prompt_es_slang_mix_v01_8x7b_instruct.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a348dee-6ae3-4261-abed-71cb09d14417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amit\n"
     ]
    }
   ],
   "source": [
    "print(\"amit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a3c98-e0a8-455e-9f5b-fd04db785424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835484f-89dc-43b6-b17a-d9ec4340e70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36449b3e-f4fe-4f6d-9be7-1cbb1969911d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca55710-bd08-46f2-843a-9e1ebdcb4c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f13dc-31f1-4168-b2f8-71e4d5240afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dataset['outputs'] = [ {\"model_name\": model_id, \"response\": output.outputs[0].text} for output in outputs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1bf2a-484e-45c4-9aac-71ab54e7c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dataset.to_excel(\"./inference/case_1/bn_prompt_bn_slang_llama3_70b_instruct.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fae4e-2e2c-4a98-9e1e-f0c5f02f13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dataset.to_excel(\"./inference/case_2/en_base_llama3_70b_instruct.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8880356-aac1-4052-b06b-0ea9cfabbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06711e2-b8a6-40a2-aa8a-a6b293de6817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"amit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451dd236-c308-4a74-9272-f2bbfbee3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ccfe0-4c20-4106-913c-b339784c39bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2b2d5-4d04-446a-9b3b-cb4048913155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981859ee-4525-4818-99c1-86334308a774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b223cc-2c14-405c-904b-07b35fb39681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f3ac2-97f7-4658-b551-11ae6c149900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "246e4b93-d21d-4a35-a895-7738f00734ba",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb67f120-e533-4157-a382-873cab37a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"can you write a new theorem in physics\",\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6639198-ebb4-4830-be65-55452a82d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e8bffe-40a3-4389-9994-10e335274fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.72s/it, est. speed input: 6.97 toks/s, output: 156.14 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " While I can certainly help you formulate a hypothesis or propose a new concept in physics, I cannot create a theorem that has been proven and widely accepted by the scientific community. However, here's a hypothetical concept that could be explored further:\n",
      "\n",
      "Conjecture: The Principle of Quantum Entanglement Conservation\n",
      "\n",
      "Statement: In a closed system of entangled quantum particles, the total entanglement entropy remains constant, regardless of the interactions and transformations within the system.\n",
      "\n",
      "Explanation: Quantum entanglement is a phenomenon where two or more particles become correlated in such a way that the state of one particle cannot be described independently of the state of the other(s). Entanglement entropy is a measure of the degree of entanglement between subsystems of a larger quantum system. This conjecture proposes that the total entanglement entropy within a closed system of entangled particles remains constant, even as individual particles become entangled or disentangled with each other. This could have implications for understanding the behavior of complex quantum systems and the nature of quantum information.\n",
      "\n",
      "Again, this is a hypothetical concept and not a proven theorem. It would require rigorous testing and mathematical proof to be considered a valid theorem in physics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.chat(messages=[messages,messages,messages], sampling_params=sampling_params)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9ec9d6b-cfaa-4022-ad42-1f258342e2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" While I can certainly help you formulate a hypothesis or propose a new concept in physics, I cannot create a theorem that has been proven and widely accepted by the scientific community. However, here's a hypothetical concept that could be explored further:\\n\\nConjecture: The Principle of Quantum Entanglement Conservation\\n\\nStatement: In a closed system of entangled quantum particles, the total entanglement entropy remains constant, regardless of the interactions and transformations within the system.\\n\\nExplanation: Quantum entanglement is a phenomenon where two or more particles become correlated in such a way that the state of one particle cannot be described independently of the state of the other(s). Entanglement entropy is a measure of the degree of entanglement between subsystems of a larger quantum system. This conjecture proposes that the total entanglement entropy within a closed system of entangled particles remains constant, even as individual particles become entangled or disentangled with each other. This could have implications for understanding the behavior of complex quantum systems and the nature of quantum information.\\n\\nAgain, this is a hypothetical concept and not a proven theorem. It would require rigorous testing and mathematical proof to be considered a valid theorem in physics.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec8d18-a974-41cd-9841-738e4d357ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60469a7-eb64-4864-a75f-615072689c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import json\n",
    "# import os\n",
    "\n",
    "def infer_models(model_name, csv_file_path, temperature=1.0, output_jsonl='output.jsonl', device=0):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load prompts from CSV into a DataFrame\n",
    "    prompts_dataset = pd.read_csv(csv_file_path, on_bad_lines='skip')\n",
    "    prompts_dataset.drop(columns=[\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "    prompts_dataset = prompts_dataset.head(2)\n",
    "    \n",
    "    # Check for existing JSONL file and determine the number of processed rows\n",
    "    processed_rows = 0\n",
    "    last_index = -1\n",
    "    if os.path.exists(output_jsonl):\n",
    "        with open(output_jsonl, 'r') as jsonl_file:\n",
    "            for line in jsonl_file:\n",
    "                entry = json.loads(line)\n",
    "                last_index = entry['index']  # Get the last processed index\n",
    "                processed_rows += 1\n",
    "        print(f\"Resuming from row {processed_rows}\")\n",
    "\n",
    "    # Initialize the model pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16, \"temperature\": temperature},\n",
    "        # device=\"cuda\",\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=1024\n",
    "    )\n",
    "\n",
    "    # Open the JSONL file in append mode\n",
    "    with open(output_jsonl, 'a') as jsonl_file:\n",
    "        # Process prompts individually, starting from the last processed row\n",
    "        for index, row in tqdm(prompts_dataset.iloc[processed_rows:].iterrows()):\n",
    "            prompt = row[\"Prompts\"]\n",
    "            message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            output = pipe(message)  # Pass the prompt directly\n",
    "            \n",
    "            # Assuming output structure, you might need to adjust this\n",
    "            resp = output[0][\"generated_text\"][-1][\"content\"] # Adjust based on your model output\n",
    "\n",
    "            # Create entry for the prompt with the index\n",
    "            entry = {\n",
    "                \"index\": index ,  # Adjust index to reflect total processed\n",
    "                **row.to_dict(),\n",
    "                \"model_response\": {\n",
    "                    \"response\": resp,\n",
    "                    \"model_name\": model_name\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Write the entry to the JSONL file\n",
    "            jsonl_file.write(json.dumps(entry) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba069837-3698-40ae-bbd0-1cefbdd35f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_models(\n",
    "    model_name=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    csv_file_path=\"./English_prompts_with_slangs_in_local_language.csv\",\n",
    "    temperature=0.000,\n",
    "    # output_jsonl='/home/hiteshlp/data_generation/multilingual_results/en_base_llama3_8b_instruct.jsonl',\n",
    "    output_jsonl='./multi_inference_results/en_base_llama3_70b_instruct.jsonl',\n",
    "    device=\"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18392200-3a02-4629-b1fe-dfc85c9e8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d1fc4-683b-4768-aa63-3158e6e2f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"amit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b6945-8a45-4983-85d8-7e348a2c7289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7408eef-2f6f-41d7-8564-35dba5b5af58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_multi",
   "language": "python",
   "name": "rag_multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
